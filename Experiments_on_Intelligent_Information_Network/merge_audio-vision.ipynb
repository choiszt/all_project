{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np                # 数组的数值计算库\nimport pandas as pd               # 数据处理和分析库\nimport matplotlib.pyplot as plt   # 数据可视化库\nimport torch                       # PyTorch深度学习库\nimport os                          # 与操作系统交互的库\nimport pytorch_lightning as pl     # 基于PyTorch的轻量级深度学习框架\nfrom torch.utils.data import Dataset, DataLoader   # PyTorch数据加载和处理库\nfrom sklearn import model_selection   # 机器学习模型选择和评估库\nimport torchvision.transforms as transforms   # PyTorch中的图像转换库\nimport torchvision.io    # PyTorch中加载和保存图像和视频文件的库\nimport librosa          # 音频处理和分析库\nfrom PIL import Image   # Python中处理图像的库\nimport albumentations as alb   # 图像增强技术库\nimport torch.multiprocessing as mp   # PyTorch中的多进程数据加载库\nimport warnings          # Python中的警告处理库\n\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":6.667352,"end_time":"2022-04-22T06:00:08.901647","exception":false,"start_time":"2022-04-22T06:00:02.234295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:09:18.525801Z","iopub.execute_input":"2023-06-05T04:09:18.526737Z","iopub.status.idle":"2023-06-05T04:09:46.725151Z","shell.execute_reply.started":"2023-06-05T04:09:18.526689Z","shell.execute_reply":"2023-06-05T04:09:46.723927Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np                # 用于数组的数值计算\nimport librosa as lb              # 音频处理和分析库\nimport librosa.display as lbd     # 用于在matplotlib中显示音频处理的库\nimport soundfile as sf            # 读写音频文件的库\nfrom soundfile import SoundFile   # 用于读取音频文件的类\nimport pandas as pd               # 数据处理和分析库\nfrom IPython.display import Audio  # 在Jupyter Notebook中播放音频的库\nfrom pathlib import Path          # 处理路径的库\n\nfrom matplotlib import pyplot as plt   # 数据可视化库\nfrom tqdm.notebook import tqdm          # 在Jupyter Notebook中显示进度条的库\nimport joblib, json, re                # 数据处理和序列化的库\n\nfrom sklearn.model_selection import StratifiedKFold  # 用于交叉验证的类\ntqdm.pandas()\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping #lightning中一些常用的回调函数","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:09:46.727873Z","iopub.execute_input":"2023-06-05T04:09:46.728268Z","iopub.status.idle":"2023-06-05T04:09:46.820749Z","shell.execute_reply.started":"2023-06-05T04:09:46.728226Z","shell.execute_reply":"2023-06-05T04:09:46.819352Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:09:46.822461Z","iopub.execute_input":"2023-06-05T04:09:46.822890Z","iopub.status.idle":"2023-06-05T04:09:46.829823Z","shell.execute_reply.started":"2023-06-05T04:09:46.822843Z","shell.execute_reply":"2023-06-05T04:09:46.828583Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def compute_melspec(y, sr, n_mels, fmin, fmax):\n    # 计算Mel频谱图\n    melspec = lb.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,)\n    # 将功率谱转换为分贝\n    melspec = lb.power_to_db(melspec).astype(np.float32)\n    return melspec","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:09:46.833994Z","iopub.execute_input":"2023-06-05T04:09:46.834810Z","iopub.status.idle":"2023-06-05T04:09:46.842549Z","shell.execute_reply.started":"2023-06-05T04:09:46.834769Z","shell.execute_reply":"2023-06-05T04:09:46.841395Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Config:\n    use_aug = False                # 是否使用数据增强\n    num_classes = 360              # 分类的类别数\n    batch_size = 64                # 批次大小\n    epochs = 5                     # 训练轮数\n    PRECISION = 16                 # 模型精度\n    PATIENCE = 8                   # EarlyStopping策略的耐心程度\n    seed = 2023                    # 随机数生成器的种子\n    model = \"tf_efficientnet_b0_ns\"   # 所使用的模型架构\n    pretrained = True              # 是否使用预训练权重\n    weight_decay = 1e-3            # 权重衰减系数\n    use_mixup = True               # 是否使用MixUp数据增强\n    mixup_alpha = 0.2              # MixUp的超参数\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 使用的设备（GPU或CPU）\n\n    data_root = \"/kaggle/input/birdclef-2023/\"   # 数据文件的根目录\n    train_images = \"/kaggle/input/split-creating-melspecs-stage-1/specs/train/\"   # 训练集数据路径\n    valid_images = \"/kaggle/input/split-creating-melspecs-stage-1/specs/valid/\"   # 验证集数据路径\n    train_path = \"/kaggle/input/bc2023-train-val-df/train.csv\"   # 训练集标签路径\n    valid_path = \"/kaggle/input/bc2023-train-val-df/valid.csv\"   # 验证集标签路径\n    \n    SR = 32000                      # 音频的采样率\n    DURATION = 5                    # 音频的时长\n    MAX_READ_SAMPLES = 5            # 最多读取的音频样本数\n    LR = 5e-4                       # 学习率\n    \n    sampling_rate = 32000           # 音频的采样率\n    duration = 5                   # 音频的时长\n    fmin = 0                        # Mel频谱图最小频率\n    fmax = None                     # Mel频谱图最大频率\n    audios_path = Path(\"/kaggle/input/birdclef-2023/train_audio\")   # 音频文件存储的路径\n    out_dir_train = Path(\"specs/train\")   # 训练集Mel频谱图输出路径\n    out_dir_valid = Path(\"specs/valid\")   # 验证集Mel频谱图输出路径\n","metadata":{"papermill":{"duration":0.099568,"end_time":"2022-04-22T06:00:18.542447","exception":false,"start_time":"2022-04-22T06:00:18.442879","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:09:46.844592Z","iopub.execute_input":"2023-06-05T04:09:46.845001Z","iopub.status.idle":"2023-06-05T04:09:46.945535Z","shell.execute_reply.started":"2023-06-05T04:09:46.844963Z","shell.execute_reply":"2023-06-05T04:09:46.943957Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#将单通道mel图convert成rgb\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n    \n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n#对信号裁剪和填充\ndef crop_or_pad(y, length, is_train=True, start=None):\n    if len(y) < length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n        \n        n_repeats = length // len(y)\n        epsilon = length % len(y)\n        \n        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n        \n    elif len(y) > length:\n        if not is_train:\n            start = start or 0\n        else:\n            start = start or np.random.randint(len(y) - length)\n\n        y = y[start:start + length]\n\n    return y","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:09:46.947803Z","iopub.execute_input":"2023-06-05T04:09:46.948833Z","iopub.status.idle":"2023-06-05T04:09:46.964749Z","shell.execute_reply.started":"2023-06-05T04:09:46.948783Z","shell.execute_reply":"2023-06-05T04:09:46.963507Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class AudioToImage:\n    def __init__(self, sr=Config.sampling_rate, n_mels=128, fmin=Config.fmin, fmax=Config.fmax, duration=Config.duration, step=None, res_type=\"kaiser_fast\", resample=True, train = True):\n\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr//2\n\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        self.step = step or self.audio_length\n        \n        self.res_type = res_type\n        self.resample = resample\n\n        self.train = train\n    def audio_to_image(self, audio):\n        melspec = compute_melspec(audio, self.sr, self.n_mels, self.fmin, self.fmax ) \n        image = mono_to_color(melspec)\n#         compute_melspec(y, sr, n_mels, fmin, fmax)\n        return image\n\n    def __call__(self, row, save=True):\n\n      audio, orig_sr = sf.read(row.path, dtype=\"float32\")\n\n      if self.resample and orig_sr != self.sr:\n        audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n        \n      audios = [audio[i:i+self.audio_length] for i in range(0, max(1, len(audio) - self.audio_length + 1), self.step)]\n      audios[-1] = crop_or_pad(audios[-1] , length=self.audio_length)\n      images = [self.audio_to_image(audio) for audio in audios]\n      images = np.stack(images)\n        \n      if save:\n        if self.train:\n            path = Config.out_dir_train/f\"{row.filename}.npy\"\n        else:\n            path = Config.out_dir_valid/f\"{row.filename}.npy\"\n            \n        path.parent.mkdir(exist_ok=True, parents=True)\n        np.save(str(path), images)\n      else:\n        return  row.filename, images\n    def vismel(self,row,idx, save=True):\n        audio, orig_sr = sf.read(row.path[idx], dtype=\"float32\")\n        if self.resample and orig_sr != self.sr:\n            audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n\n        audios = [audio[i:i+self.audio_length] for i in range(0, max(1, len(audio) - self.audio_length + 1), self.step)]\n        audios[-1] = crop_or_pad(audios[-1] , length=self.audio_length)\n        images = [self.audio_to_image(audio) for audio in audios]\n        images = np.stack(images)\n        return images\n    def get_audio(self,row,idx):\n        audio, orig_sr = sf.read(row.path[idx], dtype=\"float32\")\n        if self.resample and orig_sr != self.sr:\n            audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n        audios = [audio[i:i+self.audio_length] for i in range(0, max(1, len(audio) - self.audio_length + 1), self.step)]\n        audios[-1] = crop_or_pad(audios[-1] , length=self.audio_length)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:09:46.967456Z","iopub.execute_input":"2023-06-05T04:09:46.967782Z","iopub.status.idle":"2023-06-05T04:09:46.992581Z","shell.execute_reply.started":"2023-06-05T04:09:46.967753Z","shell.execute_reply":"2023-06-05T04:09:46.991185Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install -q torchtoolbox timm\n","metadata":{"papermill":{"duration":9.48179,"end_time":"2022-04-22T06:00:18.413844","exception":false,"start_time":"2022-04-22T06:00:08.932054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:09:46.994665Z","iopub.execute_input":"2023-06-05T04:09:46.995537Z","iopub.status.idle":"2023-06-05T04:10:01.364197Z","shell.execute_reply.started":"2023-06-05T04:09:46.995478Z","shell.execute_reply":"2023-06-05T04:10:01.362871Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"pl.seed_everything(Config.seed, workers=True)  # 使用PyTorch Lightning框架的seed_everything函数来设置随机种子，以确保实验的可重复性。","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.366651Z","iopub.execute_input":"2023-06-05T04:10:01.367109Z","iopub.status.idle":"2023-06-05T04:10:01.384292Z","shell.execute_reply.started":"2023-06-05T04:10:01.367051Z","shell.execute_reply":"2023-06-05T04:10:01.382872Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"2023"},"metadata":{}}]},{"cell_type":"code","source":"def config_to_dict(cfg):\n    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))","metadata":{"papermill":{"duration":0.033041,"end_time":"2022-04-22T06:00:18.664481","exception":false,"start_time":"2022-04-22T06:00:18.63144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:01.390397Z","iopub.execute_input":"2023-06-05T04:10:01.390802Z","iopub.status.idle":"2023-06-05T04:10:01.398387Z","shell.execute_reply.started":"2023-06-05T04:10:01.390770Z","shell.execute_reply":"2023-06-05T04:10:01.395333Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(Config.train_path)\ndf_valid = pd.read_csv(Config.valid_path)\ndf_train.head()","metadata":{"papermill":{"duration":58.466679,"end_time":"2022-04-22T06:01:17.158088","exception":false,"start_time":"2022-04-22T06:00:18.691409","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:01.399898Z","iopub.execute_input":"2023-06-05T04:10:01.400815Z","iopub.status.idle":"2023-06-05T04:10:01.611731Z","shell.execute_reply.started":"2023-06-05T04:10:01.400673Z","shell.execute_reply":"2023-06-05T04:10:01.610574Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  primary_label secondary_labels                                type  \\\n0       yebapa1               []                            ['song']   \n1       yebapa1               []                            ['song']   \n2       combuz1               []                            ['call']   \n3       chibat1      ['laudov1']  ['adult', 'sex uncertain', 'song']   \n4       carcha1               []                            ['song']   \n\n   latitude  longitude  scientific_name             common_name  \\\n0   -3.3923    36.7049   Apalis flavida  Yellow-breasted Apalis   \n1   -0.6143    34.0906   Apalis flavida  Yellow-breasted Apalis   \n2   51.8585    -8.2699      Buteo buteo          Common Buzzard   \n3  -33.1465    26.4001    Batis molitor          Chinspot Batis   \n4  -34.0110    18.8078  Cossypha caffra         Cape Robin-Chat   \n\n                  author                                            license  \\\n0           isaac kilusu  Creative Commons Attribution-NonCommercial-Sha...   \n1          James Bradley  Creative Commons Attribution-NonCommercial-Sha...   \n2  Irish Wildlife Sounds  Creative Commons Attribution-NonCommercial-Sha...   \n3         Lynette Rudman  Creative Commons Attribution-NonCommercial-Sha...   \n4      Shannon Ronaldson  Creative Commons Attribution-NonCommercial-Sha...   \n\n   rating                                url              filename  \\\n0     3.0  https://www.xeno-canto.org/422175  yebapa1/XC422175.ogg   \n1     3.0  https://www.xeno-canto.org/289562  yebapa1/XC289562.ogg   \n2     4.0  https://www.xeno-canto.org/626969  combuz1/XC626969.ogg   \n3     3.5  https://www.xeno-canto.org/664196  chibat1/XC664196.ogg   \n4     1.0  https://www.xeno-canto.org/322333  carcha1/XC322333.ogg   \n\n   len_sec_labels                                               path   frames  \\\n0               0  /kaggle/input/birdclef-2023/train_audio/yebapa...   405504   \n1               0  /kaggle/input/birdclef-2023/train_audio/yebapa...   796630   \n2               0  /kaggle/input/birdclef-2023/train_audio/combuz...   254112   \n3               1  /kaggle/input/birdclef-2023/train_audio/chibat...  1040704   \n4               0  /kaggle/input/birdclef-2023/train_audio/carcha...    40124   \n\n      sr   duration  \n0  32000  12.672000  \n1  32000  24.894687  \n2  32000   7.941000  \n3  32000  32.522000  \n4  32000   1.253875  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>primary_label</th>\n      <th>secondary_labels</th>\n      <th>type</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>scientific_name</th>\n      <th>common_name</th>\n      <th>author</th>\n      <th>license</th>\n      <th>rating</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>len_sec_labels</th>\n      <th>path</th>\n      <th>frames</th>\n      <th>sr</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>yebapa1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-3.3923</td>\n      <td>36.7049</td>\n      <td>Apalis flavida</td>\n      <td>Yellow-breasted Apalis</td>\n      <td>isaac kilusu</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.0</td>\n      <td>https://www.xeno-canto.org/422175</td>\n      <td>yebapa1/XC422175.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/yebapa...</td>\n      <td>405504</td>\n      <td>32000</td>\n      <td>12.672000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yebapa1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-0.6143</td>\n      <td>34.0906</td>\n      <td>Apalis flavida</td>\n      <td>Yellow-breasted Apalis</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.0</td>\n      <td>https://www.xeno-canto.org/289562</td>\n      <td>yebapa1/XC289562.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/yebapa...</td>\n      <td>796630</td>\n      <td>32000</td>\n      <td>24.894687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>combuz1</td>\n      <td>[]</td>\n      <td>['call']</td>\n      <td>51.8585</td>\n      <td>-8.2699</td>\n      <td>Buteo buteo</td>\n      <td>Common Buzzard</td>\n      <td>Irish Wildlife Sounds</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.0</td>\n      <td>https://www.xeno-canto.org/626969</td>\n      <td>combuz1/XC626969.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/combuz...</td>\n      <td>254112</td>\n      <td>32000</td>\n      <td>7.941000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chibat1</td>\n      <td>['laudov1']</td>\n      <td>['adult', 'sex uncertain', 'song']</td>\n      <td>-33.1465</td>\n      <td>26.4001</td>\n      <td>Batis molitor</td>\n      <td>Chinspot Batis</td>\n      <td>Lynette Rudman</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/664196</td>\n      <td>chibat1/XC664196.ogg</td>\n      <td>1</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/chibat...</td>\n      <td>1040704</td>\n      <td>32000</td>\n      <td>32.522000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>carcha1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-34.0110</td>\n      <td>18.8078</td>\n      <td>Cossypha caffra</td>\n      <td>Cape Robin-Chat</td>\n      <td>Shannon Ronaldson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>1.0</td>\n      <td>https://www.xeno-canto.org/322333</td>\n      <td>carcha1/XC322333.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/carcha...</td>\n      <td>40124</td>\n      <td>32000</td>\n      <td>1.253875</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"typelist=[]\nfor types in df_train['type']:\n    # print(eval(types))\n    for type in eval(types):\n        if type not in typelist:\n            typelist.append(type)\n# typelist #   catergories=360","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.614734Z","iopub.execute_input":"2023-06-05T04:10:01.615617Z","iopub.status.idle":"2023-06-05T04:10:01.720394Z","shell.execute_reply.started":"2023-06-05T04:10:01.615564Z","shell.execute_reply":"2023-06-05T04:10:01.719201Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Config.num_classes = len(df_train.primary_label.unique())\nConfig.num_classes=len(typelist)","metadata":{"papermill":{"duration":0.035353,"end_time":"2022-04-22T06:01:17.283888","exception":false,"start_time":"2022-04-22T06:01:17.248535","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:01.722457Z","iopub.execute_input":"2023-06-05T04:10:01.722929Z","iopub.status.idle":"2023-06-05T04:10:01.729286Z","shell.execute_reply.started":"2023-06-05T04:10:01.722882Z","shell.execute_reply":"2023-06-05T04:10:01.728214Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"multi_label=torch.zeros(360,dtype=torch.float)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.731258Z","iopub.execute_input":"2023-06-05T04:10:01.732082Z","iopub.status.idle":"2023-06-05T04:10:01.751469Z","shell.execute_reply.started":"2023-06-05T04:10:01.732040Z","shell.execute_reply":"2023-06-05T04:10:01.750386Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_train = pd.concat([df_train, pd.get_dummies(df_train['primary_label'])], axis=1)\ndf_valid = pd.concat([df_valid, pd.get_dummies(df_valid['primary_label'])], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.753373Z","iopub.execute_input":"2023-06-05T04:10:01.753813Z","iopub.status.idle":"2023-06-05T04:10:01.792260Z","shell.execute_reply.started":"2023-06-05T04:10:01.753772Z","shell.execute_reply":"2023-06-05T04:10:01.791170Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 一些数据增强策略","metadata":{}},{"cell_type":"code","source":"import albumentations as A\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.OneOf([\n                A.Cutout(max_h_size=5, max_w_size=16),\n                A.CoarseDropout(max_holes=4),\n            ], p=0.5),\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.794330Z","iopub.execute_input":"2023-06-05T04:10:01.794790Z","iopub.status.idle":"2023-06-05T04:10:01.802037Z","shell.execute_reply.started":"2023-06-05T04:10:01.794745Z","shell.execute_reply":"2023-06-05T04:10:01.800531Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"    def __init__(self, sr=Config.sampling_rate, n_mels=128, fmin=Config.fmin, fmax=Config.fmax, duration=Config.duration, step=None, res_type=\"kaiser_fast\", resample=True, train = True):\n\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr//2\n\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        self.step = step or self.audio_length\n        \n        self.res_type = res_type\n        self.resample = resample\n\n        self.train = train","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.804433Z","iopub.execute_input":"2023-06-05T04:10:01.804927Z","iopub.status.idle":"2023-06-05T04:10:01.816225Z","shell.execute_reply.started":"2023-06-05T04:10:01.804886Z","shell.execute_reply":"2023-06-05T04:10:01.814820Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### **Utility**","metadata":{}},{"cell_type":"code","source":"# Generates random integer\ndef random_int(shape=[], minval=0, maxval=1):\n    return tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.int32)\n\n\n# Generats random float\ndef random_float(shape=[], minval=0.0, maxval=1.0):\n    rnd = tf.random.uniform(shape=shape, minval=minval, maxval=maxval, dtype=tf.float32)\n    return rnd","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.817901Z","iopub.execute_input":"2023-06-05T04:10:01.818698Z","iopub.status.idle":"2023-06-05T04:10:01.830245Z","shell.execute_reply.started":"2023-06-05T04:10:01.818661Z","shell.execute_reply":"2023-06-05T04:10:01.829447Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# **some augmentation trick concerning audio**","metadata":{}},{"cell_type":"code","source":"# Import required packages\nimport tensorflow as tf\n\n# Define a function to crop or pad audio data to a target length\n@tf.function\ndef CropOrPad(audio, target_len, pad_mode='constant'):\n    # Get the length of the input audio\n    audio_len = tf.shape(audio)[0]\n    # If the length of the input audio is smaller than the target length, randomly pad the audio\n    if audio_len < target_len:\n        # Calculate the offset between the input audio and the target length\n        diff_len = (target_len - audio_len)\n        # Select a random location for padding\n        pad1 = random_int([], minval=0, maxval=diff_len)\n        # Calculate the second padding value\n        pad2 = diff_len - pad1\n        pad_len = [pad1, pad2]\n        # Apply padding to the audio data\n        audio = tf.pad(audio, paddings=[pad_len], mode=pad_mode)\n    # If the length of the input audio is larger than the target length, crop the audio\n    elif audio_len > target_len:\n        # Calculate the difference in length between the input audio and the target length\n        diff_len = (audio_len - target_len)\n        # Select a random location for cropping\n        idx = tf.random.uniform([], 0, diff_len, dtype=tf.int32)\n        # Crop the audio data\n        audio = audio[idx: (idx + target_len)]\n    # Reshape the audio data to the target length\n    audio = tf.reshape(audio, [target_len])\n    # Return the cropped or padded audio data\n    return audio\n\n\n# Randomly shift audio -> any sound at <t> time may get shifted to <t+shift> time\n@tf.function\ndef TimeShift(audio, prob=0.5):\n    # Randomly apply time shift with probability `prob`\n    if random_float() < prob:\n        # Calculate random shift value\n        shift = random_int(shape=[], minval=0, maxval=tf.shape(audio)[0])\n        # Randomly set the shift to be negative with 50% probability\n        if random_float() < 0.5:\n            shift = -shift\n        # Roll the audio signal by the shift value\n        audio = tf.roll(audio, shift, axis=0)\n    return audio\n\n# Apply random noise to audio data\n@tf.function\ndef GaussianNoise(audio, std=[0.0025, 0.025], prob=0.5):\n    # Select a random value of standard deviation for Gaussian noise within the given range\n    std = random_float([], std[0], std[1])\n    # Randomly apply Gaussian noise with probability `prob`\n    if random_float() < prob:\n        # Add random Gaussian noise to the audio signal\n        GN = tf.keras.layers.GaussianNoise(stddev=std)\n        audio = GN(audio, training=True) # training=False don't apply noise to data\n    return audio\n\n# Applies augmentation to Audio Signal\ndef AudioAug(audio):\n    # Apply time shift and Gaussian noise to the audio signal\n    audio = TimeShift(audio, prob=CFG.timeshift_prob)\n    audio = GaussianNoise(audio, prob=CFG.gn_prob)\n    return audio\n\n# Standardize the audio\n@tf.function\ndef Normalize(data, min_max=True):\n    # Compute the mean and standard deviation of the data\n    MEAN = tf.math.reduce_mean(data)\n    STD = tf.math.reduce_std(data)\n    # Standardize the data\n    data = tf.math.divide_no_nan(data - MEAN, STD)\n    # Normalize to [0, 1]\n    if min_max:\n        MIN = tf.math.reduce_min(data)\n        MAX = tf.math.reduce_max(data)\n        data = tf.math.divide_no_nan(data - MIN, MAX - MIN)\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.832202Z","iopub.execute_input":"2023-06-05T04:10:01.833237Z","iopub.status.idle":"2023-06-05T04:10:01.853916Z","shell.execute_reply.started":"2023-06-05T04:10:01.833191Z","shell.execute_reply":"2023-06-05T04:10:01.852819Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"ass BirdDataset_formerge(torch.utils.data.Dataset):\n    def __init__(self, df, multi_label=multi_label, sr=Config.SR, duration=Config.DURATION, augmentations=None, train=True, n_mels=128, fmin=Config.fmin, fmax=Config.fmax, step=None):\n        # 初始化函数，设置一些参数\n        self.n_mels = n_mels   # Mel频谱图的维度数\n        self.df = df   # 数据集的DataFrame\n        self.sr = sr   # 音频的采样率\n        self.train = train   # 是否是训练集\n        self.duration = duration   # 音频的时长\n        self.augmentations = augmentations   # 数据增强方法\n        self.labels = multi_label   # 标签是否为多标签\n        self.fmin = fmin   # Mel频谱图最小频率\n        self.fmax = fmax or self.sr // 2   # Mel频谱图最大频率\n        self.duration = duration   # 音频的时长\n        self.audio_length = self.duration * self.sr   # 音频的长度\n        self.step = step or self.audio_length   # 分割音频的步长，默认为音频长度\n        self.tf2torch = lambda x: torch.tensor(x.numpy())   # 转换函数\n        if train:\n            self.img_dir = Config.train_images   # 训练集Mel频谱图的路径\n        else:\n            self.img_dir = Config.valid_images   # 验证集Mel频谱图的路径\n\n    def __len__(self):\n        # 返回数据集的长度\n        return len(self.df)\n\n    @staticmethod\n    def normalize(image):\n        image = image / 255.0\n        #image = torch.stack([image, image, image])\n        return image\n\ndef __getitem__(self, idx):\n    # 获取数据集中的一条数据\n    row = self.df.iloc[idx]\n    # 获取音频文件路径并读取音频文件\n    audiopath=row.path\n    audio, orig_sr = sf.read(audiopath, dtype=\"float32\")\n    # 将音频文件根据设定的长度和步长分割成多个子音频\n    audios = [audio[i:i+self.audio_length] for i in range(0, max(1, len(audio) - self.audio_length + 1), self.step)]\n    # 如果最后一个子音频的长度不足设定的长度，则用0进行填充\n    audios[-1] = crop_or_pad(audios[-1] , length=self.audio_length)\n    # 进行时间偏移增强\n    audio=TimeShift(audios[0])\n    # 将音频切分成三个部分，并将每个部分转换为Mel频谱图\n    audio_1=self.tf2torch(audio[0*128*313:1*128*313]).reshape(128,313)\n    audio_2=self.tf2torch(audio[1*128*313:2*128*313]).reshape(128,313)\n    audio_3=self.tf2torch(audio[2*128*313:3*128*313]).reshape(128,313)\n    # 获取对应的Mel频谱图\n    impath = self.img_dir + f\"{row.filename}.npy\"\n    image = np.load(str(impath))[:Config.MAX_READ_SAMPLES]\n    \n    ########## RANDOM SAMPLING ################\n    # 如果是训练集，则从多张Mel频谱图中随机选择一张\n    if self.train:\n        image = image[np.random.choice(len(image))]\n    # 如果是验证集，则只选择一张Mel频谱图\n    else:\n        image = image[0]\n    #####################################################################\n    \n    # 将Mel频谱图转换为PyTorchtensor类型\n    image = torch.tensor(image).float()\n    # 进行数据增强\n    if self.augmentations:\n        image = self.augmentations(image.unsqueeze(0)).squeeze()\n        \n    # 将标签转换为独热编码\n    self.tmplabel=self.labels.clone()\n    types=eval(row[2])\n    indexes=[]\n    for typename in types:\n        if typename not in typelist:\n            indexes.append(0)\n        else:\n            indexes.append(typelist.index(typename))\n    for index in indexes:\n        self.tmplabel[index]=torch.tensor(1)\n    # 将图像转换为RGB格式，并进行归一化\n    image = torch.stack([image, image, image])\n    image = self.normalize(image)\n    # 将音频和图像融合在一起\n    merged_audio=torch.stack([audio_1,audio_2,audio_3])\n    merged_image=merged_audio+image\n    # 返回融合后的数据和对应的标签\n    return merged_image, self.tmplabel","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:01.856129Z","iopub.execute_input":"2023-06-05T04:10:01.856598Z","iopub.status.idle":"2023-06-05T04:10:01.882168Z","shell.execute_reply.started":"2023-06-05T04:10:01.856556Z","shell.execute_reply":"2023-06-05T04:10:01.881003Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def get_fold_dls(df_train, df_valid):\n    # 创建训练集和验证集的数据集\n    ds_train = BirdDataset_formerge(\n        df_train, \n        sr = Config.SR,\n        duration = Config.DURATION,\n        augmentations = None,\n        train = True\n    )\n    ds_val = BirdDataset_formerge(\n        df_valid, \n        sr = Config.SR,\n        duration = Config.DURATION,\n        augmentations = None,\n        train = False\n    )\n    # 创建训练集和验证集的数据加载器\n    dl_train = DataLoader(ds_train, batch_size=Config.batch_size , shuffle=True, num_workers = 2)    \n    dl_val = DataLoader(ds_val, batch_size=Config.batch_size, num_workers = 2)\n    # 返回训练集和验证集的数据加载器以及对应的数据集\n    return dl_train, dl_val, ds_train, ds_val","metadata":{"papermill":{"duration":0.036289,"end_time":"2022-04-22T06:01:17.539606","exception":false,"start_time":"2022-04-22T06:01:17.503317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:36.113029Z","iopub.execute_input":"2023-06-05T04:10:36.114151Z","iopub.status.idle":"2023-06-05T04:10:36.127295Z","shell.execute_reply.started":"2023-06-05T04:10:36.114103Z","shell.execute_reply":"2023-06-05T04:10:36.126245Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n    # 创建一个大小为12x6的画布\n    fig = plt.figure(figsize=(12, 6))    \n    # 随机选择num_items个数据进行展示\n    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n    # 遍历每个选择的数据，并将其展示在画布上\n    for index, img_index in enumerate(img_index):  # list first 9 images\n        # 获取数据和对应的标签\n        img, lb = img_ds[img_index]        \n        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n        # 将数据转换为numpy数组，并将通道维度放到最后一维，然后显示在画布上\n        if isinstance(img, torch.Tensor):\n            img = img.detach().numpy()\n        if isinstance(img, np.ndarray):\n            img = img.transpose(1, 2, 0)\n            ax.imshow(img)        \n        # 设置标题为\"Spec\"\n        title = f\"Spec\"\n        ax.set_title(title)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:36.838460Z","iopub.execute_input":"2023-06-05T04:10:36.838950Z","iopub.status.idle":"2023-06-05T04:10:36.857780Z","shell.execute_reply.started":"2023-06-05T04:10:36.838905Z","shell.execute_reply":"2023-06-05T04:10:36.856579Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dl_train, dl_val, ds_train, ds_val = get_fold_dls(df_train, df_valid)","metadata":{"papermill":{"duration":0.584852,"end_time":"2022-04-22T06:01:18.338238","exception":false,"start_time":"2022-04-22T06:01:17.753386","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:37.503951Z","iopub.execute_input":"2023-06-05T04:10:37.504416Z","iopub.status.idle":"2023-06-05T04:10:37.510757Z","shell.execute_reply.started":"2023-06-05T04:10:37.504374Z","shell.execute_reply":"2023-06-05T04:10:37.509557Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n\ndef get_optimizer(lr, params):\n    # 创建Adam优化器，并设置学习率、权重衰减等参数\n    model_optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, params), \n            lr=lr,\n            weight_decay=Config.weight_decay\n        )\n    # 设置学习率调度器为CosineAnnealingWarmRestarts，并设置相关参数\n    interval = \"epoch\"\n    lr_scheduler = CosineAnnealingWarmRestarts(\n                            model_optimizer, \n                            T_0=Config.epochs, \n                            T_mult=1, \n                            eta_min=1e-6, \n                            last_epoch=-1\n                        )\n\n    # 返回优化器和学习率调度器的参数\n    return {\n        \"optimizer\": model_optimizer, \n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            \"interval\": interval,\n            \"monitor\": \"val_loss\",\n            \"frequency\": 1\n        }\n    }","metadata":{"papermill":{"duration":0.048043,"end_time":"2022-04-22T06:01:22.109544","exception":false,"start_time":"2022-04-22T06:01:22.061501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:42.972901Z","iopub.execute_input":"2023-06-05T04:10:42.973401Z","iopub.status.idle":"2023-06-05T04:10:42.989166Z","shell.execute_reply.started":"2023-06-05T04:10:42.973359Z","shell.execute_reply":"2023-06-05T04:10:42.987847Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from torchtoolbox.tools import mixup_data, mixup_criterion\nimport torch.nn as nn\nfrom torch.nn.functional import cross_entropy\nimport torchmetrics\nimport timm","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:43.583164Z","iopub.execute_input":"2023-06-05T04:10:43.583638Z","iopub.status.idle":"2023-06-05T04:10:44.115068Z","shell.execute_reply.started":"2023-06-05T04:10:43.583592Z","shell.execute_reply":"2023-06-05T04:10:44.113797Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics\n\ndef padded_cmap(solution, submission, padding_factor=5):\n    # 对solution和submission进行处理，增加padding_factor个全1的行\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    # 计算padded_solution和padded_submission的平均精度得分\n    score = sklearn.metrics.average_precision_score(\n        padded_solution.values,\n        padded_submission.values,\n        average='macro',\n    )\n    return score\n\ndef map_score(solution, submission):\n    # 对solution和submission进行处理，计算原始的平均精度得分\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    score = sklearn.metrics.average_precision_score(\n        solution.values,\n        submission.values,\n        average='micro',\n    )\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-06-05T04:10:44.236741Z","iopub.execute_input":"2023-06-05T04:10:44.237273Z","iopub.status.idle":"2023-06-05T04:10:44.249775Z","shell.execute_reply.started":"2023-06-05T04:10:44.237220Z","shell.execute_reply":"2023-06-05T04:10:44.248687Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class BirdClefModel(pl.LightningModule):\n    def __init__(self, model_name=Config.model, num_classes = Config.num_classes, pretrained = Config.pretrained):\n        super().__init__()\n        self.num_classes = num_classes\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if 'res' in model_name:\n            self.in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Linear(self.in_features, num_classes)\n        elif 'dense' in model_name:\n            self.in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Linear(self.in_features, num_classes)\n        elif 'efficientnet' in model_name:\n            self.in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Sequential(\n                nn.Linear(self.in_features, num_classes)\n            )\n#         print(num_classes)\n        self.loss_function = nn.BCEWithLogitsLoss() \n\n    def forward(self,images):\n        logits = self.backbone(images)\n#         print(logits.shape)\n        return logits\n        \n    def configure_optimizers(self):\n        return get_optimizer(lr=Config.LR, params=self.parameters())\n\n    def train_with_mixup(self, X, y):\n        X, y_a, y_b, lam = mixup_data(X, y, alpha=Config.mixup_alpha)\n        y_pred = self(X)\n        loss_mixup = mixup_criterion(cross_entropy, y_pred, y_a, y_b, lam)\n        return loss_mixup\n\n    def training_step(self, batch, batch_idx):\n        image, target = batch\n#         print(Config.use_mixup)\n        if Config.use_mixup:\n            loss = self.train_with_mixup(image, target)\n        else:\n            y_pred = self(image)\n            loss = self.loss_function(y_pred,target)\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        return loss        \n\n    def validation_step(self, batch, batch_idx):\n        image, target = batch     \n        y_pred = self(image)\n        val_loss = self.loss_function(y_pred, target)\n        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        \n        return {\"val_loss\": val_loss, \"logits\": y_pred, \"targets\": target}\n    \n    def train_dataloader(self):\n        return self._train_dataloader \n    \n    def validation_dataloader(self):\n        return self._validation_dataloader\n    \n    def validation_epoch_end(self,outputs):\n        \n        \n        return {'val_loss': avg_loss,'val_cmap':0}\n    \n    \n    \n    ","metadata":{"papermill":{"duration":0.156714,"end_time":"2022-04-22T06:01:22.301564","exception":false,"start_time":"2022-04-22T06:01:22.14485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:44.982644Z","iopub.execute_input":"2023-06-05T04:10:44.983216Z","iopub.status.idle":"2023-06-05T04:10:45.003900Z","shell.execute_reply.started":"2023-06-05T04:10:44.983175Z","shell.execute_reply":"2023-06-05T04:10:45.002857Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nimport gc\n\ndef run_training():\n    print(f\"Running training...\")\n    logger = None\n    \n    # 获得训练集和验证集的dataloader以及dataset\n    dl_train, dl_val, ds_train, ds_val = get_fold_dls(df_train, df_valid)\n    \n    # 构建模型\n    audio_model = BirdClefModel()\n\n    # 设置早停和保存模型的回调函数\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=Config.PATIENCE, verbose= True, mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                          dirpath= \"/kaggle/working/exp1/\",\n                                      save_top_k=1,\n                                      save_last= True,\n                                      save_weights_only=True,\n                                      filename= f'./{Config.model}_loss',\n                                      verbose= True,\n                                      mode='min')\n    \n    callbacks_to_use = [checkpoint_callback,early_stop_callback]\n\n    # 构建Trainer对象，并设置相关参数\n    trainer = pl.Trainer(\n        gpus=1,\n        val_check_interval=0.5,\n        deterministic=True,\n        max_epochs=Config.epochs,\n        logger=logger,\n        auto_lr_find=False,    \n        callbacks=callbacks_to_use,\n        precision=Config.PRECISION, accelerator=\"gpu\" \n    )\n\n    # 调用trainer.fit方法进行训练\n    print(\"Running trainer.fit\")\n    trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)                \n\n    # 回收内存和清空GPU缓存\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"papermill":{"duration":0.052364,"end_time":"2022-04-22T06:01:22.708806","exception":false,"start_time":"2022-04-22T06:01:22.656442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-05T04:10:45.956371Z","iopub.execute_input":"2023-06-05T04:10:45.957108Z","iopub.status.idle":"2023-06-05T04:10:45.967410Z","shell.execute_reply.started":"2023-06-05T04:10:45.957067Z","shell.execute_reply":"2023-06-05T04:10:45.966303Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}