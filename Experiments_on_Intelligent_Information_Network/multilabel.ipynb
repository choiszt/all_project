{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport os\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn import model_selection\nimport torchvision.transforms as transforms\nimport torchvision.io \nimport librosa\nfrom PIL import Image\nimport albumentations as alb\nimport torch.multiprocessing as mp\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":6.667352,"end_time":"2022-04-22T06:00:08.901647","exception":false,"start_time":"2022-04-22T06:00:02.234295","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:45:33.558079Z","iopub.execute_input":"2023-06-08T08:45:33.558431Z","iopub.status.idle":"2023-06-08T08:45:49.024415Z","shell.execute_reply.started":"2023-06-08T08:45:33.558399Z","shell.execute_reply":"2023-06-08T08:45:49.023292Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping #回调函数\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:45:49.026731Z","iopub.execute_input":"2023-06-08T08:45:49.027106Z","iopub.status.idle":"2023-06-08T08:45:49.033953Z","shell.execute_reply.started":"2023-06-08T08:45:49.027064Z","shell.execute_reply":"2023-06-08T08:45:49.032766Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -q torchtoolbox timm\n","metadata":{"papermill":{"duration":9.48179,"end_time":"2022-04-22T06:00:18.413844","exception":false,"start_time":"2022-04-22T06:00:08.932054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:45:49.035677Z","iopub.execute_input":"2023-06-08T08:45:49.036351Z","iopub.status.idle":"2023-06-08T08:46:01.975911Z","shell.execute_reply.started":"2023-06-08T08:45:49.036312Z","shell.execute_reply":"2023-06-08T08:46:01.974641Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"class Config:\n    use_aug = False  # 是否使用数据增强\n    num_classes = 360  # 分类任务的类别数量\n    batch_size = 64  # 每个batch的样本数\n    epochs = 5  # 训练的轮数\n    PRECISION = 16  # 训练时使用的精度\n    PATIENCE = 8  # 早停的最大等待轮数\n    seed = 2023  # 随机数生成器的种子\n    model = \"tf_efficientnet_b0_ns\"  # 使用的模型的名称\n    pretrained = True  # 是否使用预训练权重\n    weight_decay = 1e-3  # 权重衰减的系数\n    use_mixup = True  # 是否使用Mixup数据增强\n    mixup_alpha = 0.2  # Mixup数据增强的超参数\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 设置使用的设备\n\n    data_root = \"/kaggle/input/birdclef-2023/\"  # 数据集存放的根目录\n    train_images = \"/kaggle/input/split-creating-melspecs-stage-1/specs/train/\"  # 训练集图像存放的路径\n    valid_images = \"/kaggle/input/split-creating-melspecs-stage-1/specs/valid/\"  # 验证集图像存放的路径\n    train_path = \"/kaggle/input/bc2023-train-val-df/train.csv\"  # 训练集元数据文件的路径\n    valid_path = \"/kaggle/input/bc2023-train-val-df/valid.csv\"  # 验证集元数据文件的路径\n    \n    SR = 32000  # 音频的采样率\n    DURATION = 5  # 音频的时长\n    MAX_READ_SAMPLES = 5  # 读取音频时最多读取的帧数\n    LR = 5e-4  # 初始学习率","metadata":{"papermill":{"duration":0.099568,"end_time":"2022-04-22T06:00:18.542447","exception":false,"start_time":"2022-04-22T06:00:18.442879","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:01.980587Z","iopub.execute_input":"2023-06-08T08:46:01.981135Z","iopub.status.idle":"2023-06-08T08:46:02.054511Z","shell.execute_reply.started":"2023-06-08T08:46:01.981091Z","shell.execute_reply":"2023-06-08T08:46:02.052754Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(Config.seed, workers=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.056552Z","iopub.execute_input":"2023-06-08T08:46:02.056961Z","iopub.status.idle":"2023-06-08T08:46:02.071439Z","shell.execute_reply.started":"2023-06-08T08:46:02.056922Z","shell.execute_reply":"2023-06-08T08:46:02.070173Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2023"},"metadata":{}}]},{"cell_type":"code","source":"def config_to_dict(cfg):\n    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))","metadata":{"papermill":{"duration":0.033041,"end_time":"2022-04-22T06:00:18.664481","exception":false,"start_time":"2022-04-22T06:00:18.63144","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:02.073228Z","iopub.execute_input":"2023-06-08T08:46:02.073593Z","iopub.status.idle":"2023-06-08T08:46:02.081001Z","shell.execute_reply.started":"2023-06-08T08:46:02.073555Z","shell.execute_reply":"2023-06-08T08:46:02.079890Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(Config.train_path)\ndf_valid = pd.read_csv(Config.valid_path)\ndf_train.head()","metadata":{"papermill":{"duration":58.466679,"end_time":"2022-04-22T06:01:17.158088","exception":false,"start_time":"2022-04-22T06:00:18.691409","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:02.082436Z","iopub.execute_input":"2023-06-08T08:46:02.083122Z","iopub.status.idle":"2023-06-08T08:46:02.394300Z","shell.execute_reply.started":"2023-06-08T08:46:02.083092Z","shell.execute_reply":"2023-06-08T08:46:02.393012Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  primary_label secondary_labels                                type  \\\n0       yebapa1               []                            ['song']   \n1       yebapa1               []                            ['song']   \n2       combuz1               []                            ['call']   \n3       chibat1      ['laudov1']  ['adult', 'sex uncertain', 'song']   \n4       carcha1               []                            ['song']   \n\n   latitude  longitude  scientific_name             common_name  \\\n0   -3.3923    36.7049   Apalis flavida  Yellow-breasted Apalis   \n1   -0.6143    34.0906   Apalis flavida  Yellow-breasted Apalis   \n2   51.8585    -8.2699      Buteo buteo          Common Buzzard   \n3  -33.1465    26.4001    Batis molitor          Chinspot Batis   \n4  -34.0110    18.8078  Cossypha caffra         Cape Robin-Chat   \n\n                  author                                            license  \\\n0           isaac kilusu  Creative Commons Attribution-NonCommercial-Sha...   \n1          James Bradley  Creative Commons Attribution-NonCommercial-Sha...   \n2  Irish Wildlife Sounds  Creative Commons Attribution-NonCommercial-Sha...   \n3         Lynette Rudman  Creative Commons Attribution-NonCommercial-Sha...   \n4      Shannon Ronaldson  Creative Commons Attribution-NonCommercial-Sha...   \n\n   rating                                url              filename  \\\n0     3.0  https://www.xeno-canto.org/422175  yebapa1/XC422175.ogg   \n1     3.0  https://www.xeno-canto.org/289562  yebapa1/XC289562.ogg   \n2     4.0  https://www.xeno-canto.org/626969  combuz1/XC626969.ogg   \n3     3.5  https://www.xeno-canto.org/664196  chibat1/XC664196.ogg   \n4     1.0  https://www.xeno-canto.org/322333  carcha1/XC322333.ogg   \n\n   len_sec_labels                                               path   frames  \\\n0               0  /kaggle/input/birdclef-2023/train_audio/yebapa...   405504   \n1               0  /kaggle/input/birdclef-2023/train_audio/yebapa...   796630   \n2               0  /kaggle/input/birdclef-2023/train_audio/combuz...   254112   \n3               1  /kaggle/input/birdclef-2023/train_audio/chibat...  1040704   \n4               0  /kaggle/input/birdclef-2023/train_audio/carcha...    40124   \n\n      sr   duration  \n0  32000  12.672000  \n1  32000  24.894687  \n2  32000   7.941000  \n3  32000  32.522000  \n4  32000   1.253875  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>primary_label</th>\n      <th>secondary_labels</th>\n      <th>type</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>scientific_name</th>\n      <th>common_name</th>\n      <th>author</th>\n      <th>license</th>\n      <th>rating</th>\n      <th>url</th>\n      <th>filename</th>\n      <th>len_sec_labels</th>\n      <th>path</th>\n      <th>frames</th>\n      <th>sr</th>\n      <th>duration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>yebapa1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-3.3923</td>\n      <td>36.7049</td>\n      <td>Apalis flavida</td>\n      <td>Yellow-breasted Apalis</td>\n      <td>isaac kilusu</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.0</td>\n      <td>https://www.xeno-canto.org/422175</td>\n      <td>yebapa1/XC422175.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/yebapa...</td>\n      <td>405504</td>\n      <td>32000</td>\n      <td>12.672000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yebapa1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-0.6143</td>\n      <td>34.0906</td>\n      <td>Apalis flavida</td>\n      <td>Yellow-breasted Apalis</td>\n      <td>James Bradley</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.0</td>\n      <td>https://www.xeno-canto.org/289562</td>\n      <td>yebapa1/XC289562.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/yebapa...</td>\n      <td>796630</td>\n      <td>32000</td>\n      <td>24.894687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>combuz1</td>\n      <td>[]</td>\n      <td>['call']</td>\n      <td>51.8585</td>\n      <td>-8.2699</td>\n      <td>Buteo buteo</td>\n      <td>Common Buzzard</td>\n      <td>Irish Wildlife Sounds</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>4.0</td>\n      <td>https://www.xeno-canto.org/626969</td>\n      <td>combuz1/XC626969.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/combuz...</td>\n      <td>254112</td>\n      <td>32000</td>\n      <td>7.941000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chibat1</td>\n      <td>['laudov1']</td>\n      <td>['adult', 'sex uncertain', 'song']</td>\n      <td>-33.1465</td>\n      <td>26.4001</td>\n      <td>Batis molitor</td>\n      <td>Chinspot Batis</td>\n      <td>Lynette Rudman</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>3.5</td>\n      <td>https://www.xeno-canto.org/664196</td>\n      <td>chibat1/XC664196.ogg</td>\n      <td>1</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/chibat...</td>\n      <td>1040704</td>\n      <td>32000</td>\n      <td>32.522000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>carcha1</td>\n      <td>[]</td>\n      <td>['song']</td>\n      <td>-34.0110</td>\n      <td>18.8078</td>\n      <td>Cossypha caffra</td>\n      <td>Cape Robin-Chat</td>\n      <td>Shannon Ronaldson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n      <td>1.0</td>\n      <td>https://www.xeno-canto.org/322333</td>\n      <td>carcha1/XC322333.ogg</td>\n      <td>0</td>\n      <td>/kaggle/input/birdclef-2023/train_audio/carcha...</td>\n      <td>40124</td>\n      <td>32000</td>\n      <td>1.253875</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"typelist=[]\nfor types in df_train['type']:\n    # print(eval(types))\n    for type in eval(types):\n        if type not in typelist:\n            typelist.append(type)\n# typelist #   catergories=360","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.399015Z","iopub.execute_input":"2023-06-08T08:46:02.406729Z","iopub.status.idle":"2023-06-08T08:46:02.560139Z","shell.execute_reply.started":"2023-06-08T08:46:02.406664Z","shell.execute_reply":"2023-06-08T08:46:02.559043Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"multi_label=torch.zeros(360,dtype=torch.float)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.615095Z","iopub.execute_input":"2023-06-08T08:46:02.617389Z","iopub.status.idle":"2023-06-08T08:46:02.632487Z","shell.execute_reply.started":"2023-06-08T08:46:02.617351Z","shell.execute_reply":"2023-06-08T08:46:02.631491Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train = pd.concat([df_train, pd.get_dummies(df_train['primary_label'])], axis=1)\ndf_valid = pd.concat([df_valid, pd.get_dummies(df_valid['primary_label'])], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.636914Z","iopub.execute_input":"2023-06-08T08:46:02.637528Z","iopub.status.idle":"2023-06-08T08:46:02.683181Z","shell.execute_reply.started":"2023-06-08T08:46:02.637491Z","shell.execute_reply":"2023-06-08T08:46:02.682123Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Create & Fill birds with 0 samples in validation","metadata":{}},{"cell_type":"code","source":"birds = list(df_train.primary_label.unique())","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.687761Z","iopub.execute_input":"2023-06-08T08:46:02.690181Z","iopub.status.idle":"2023-06-08T08:46:02.699213Z","shell.execute_reply.started":"2023-06-08T08:46:02.690141Z","shell.execute_reply":"2023-06-08T08:46:02.697905Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"missing_birds = list(set(list(df_train.primary_label.unique())).difference(list(df_valid.primary_label.unique())))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.704300Z","iopub.execute_input":"2023-06-08T08:46:02.706954Z","iopub.status.idle":"2023-06-08T08:46:02.717143Z","shell.execute_reply.started":"2023-06-08T08:46:02.706912Z","shell.execute_reply":"2023-06-08T08:46:02.716077Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"non_missing_birds = list(set(list(df_train.primary_label.unique())).difference(missing_birds))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.720842Z","iopub.execute_input":"2023-06-08T08:46:02.721719Z","iopub.status.idle":"2023-06-08T08:46:02.731767Z","shell.execute_reply.started":"2023-06-08T08:46:02.721664Z","shell.execute_reply":"2023-06-08T08:46:02.729238Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(non_missing_birds)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.738830Z","iopub.execute_input":"2023-06-08T08:46:02.739686Z","iopub.status.idle":"2023-06-08T08:46:02.749598Z","shell.execute_reply.started":"2023-06-08T08:46:02.739640Z","shell.execute_reply":"2023-06-08T08:46:02.746833Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"254"},"metadata":{}}]},{"cell_type":"code","source":"df_valid[missing_birds] = 0\ndf_valid = df_valid[df_train.columns] ## Fix order","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.752622Z","iopub.execute_input":"2023-06-08T08:46:02.754456Z","iopub.status.idle":"2023-06-08T08:46:02.779356Z","shell.execute_reply.started":"2023-06-08T08:46:02.754417Z","shell.execute_reply":"2023-06-08T08:46:02.777806Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.OneOf([\n                A.Cutout(max_h_size=5, max_w_size=16),\n                A.CoarseDropout(max_holes=4),\n            ], p=0.5),\n    ])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.794320Z","iopub.execute_input":"2023-06-08T08:46:02.795101Z","iopub.status.idle":"2023-06-08T08:46:02.806334Z","shell.execute_reply.started":"2023-06-08T08:46:02.795064Z","shell.execute_reply":"2023-06-08T08:46:02.805156Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class BirdDataset(torch.utils.data.Dataset):\n    # 定义一个名为BirdDataset的类，继承自PyTorch中的Dataset类\n\n    def __init__(self, df,multi_label=multi_label,sr = Config.SR, duration = Config.DURATION, augmentations = None, train = True):\n        # 初始化函数，接收数据框、多标签、采样率、持续时间、数据增强和训练模式等参数\n\n        self.df = df\n        # 存储输入的数据框\n\n        self.sr = sr \n        # 存储采样率\n\n        self.train = train\n        # 存储训练模式\n\n        self.duration = duration\n        # 存储持续时间\n\n        self.augmentations = augmentations\n        # 存储数据增强\n\n        self.labels=multi_label\n        # 存储多标签\n\n        if train:\n            self.img_dir = Config.train_images\n        else:\n            self.img_dir = Config.valid_images\n        # 根据训练模式选择数据集的路径\n\n    def __len__(self):\n        return len(self.df)\n    # 返回数据集中的样本数量\n\n    @staticmethod\n    def normalize(image):\n        image = image / 255.0\n        # 对图像进行归一化处理，将像素值缩放到0-1之间\n        # 这里的image是一个numpy数组\n\n        #image = torch.stack([image, image, image])\n        # 对图像进行扩展，将其从1通道扩展为3通道，因为后续的预训练模型需要输入3通道图像\n\n        return image\n\n    def __getitem__(self, idx):\n        # 根据索引获取数据集中的一个样本\n\n        row = self.df.iloc[idx]\n        # 获取数据集中的一行数据\n\n        impath = self.img_dir + f\"{row.filename}.npy\"\n        # 构造数据集中的样本路径，这里的样本是以npy文件格式存储的\n\n        image = np.load(str(impath))[:Config.MAX_READ_SAMPLES]\n        # 使用numpy库从npy文件中加载图像数据，这里只读取前MAX_READ_SAMPLES个样本\n\n        ########## RANDOM SAMPLING ################\n        if self.train:\n            image = image[np.random.choice(len(image))]\n        else:\n            image = image[0]\n        # 对于训练模式，随机选择一个样本作为训练数据；对于测试模式，选择第一个样本作为测试数据\n        # 这里的image是一个numpy数组\n\n        #####################################################################\n\n        image = torch.tensor(image).float()\n        # 将numpy数组转换为PyTorch张量，并将其数据类型设置为float类型\n\n        if self.augmentations:\n            image = self.augmentations(image.unsqueeze(0)).squeeze()\n        # 如果存在数据增强，则对图像进行增强处理，这里使用了PyTorch中的transforms库实现数据增强\n        # 这里的image是一个PyTorch张量\n\n        image.size()\n        # 输出图像的大小\n\n        self.tmplabel=self.labels.clone()\n        # 复制多标签，以便后续修改标签\n\n        types=eval(row[2])\n        # 从数据框中获取图像的标签，并使用eval()函数将其转换为Python列表\n\n        indexes=[]\n        for typename in types:\n            if typename not in typelist:\n                indexes.append(0)\n            else:\n#             print(typename)\n                indexes.append(typelist.index(typename))\n        # 将标签转换为标签索引，如果标签不在标签列表中，则将其索引设置为0\n\n        for index in indexes:\n            self.tmplabel[index]=torch.tensor(1)\n        # 将标签索引设置为1，表示该样本包含该标签\n\n        image = torch.stack([image, image, image])\n        # 对图像进行扩展，将其从1通道扩展为3通道，因为后续的预训练模型需要输入3通道图像\n\n        image =self.normalize(image)\n        # 对图像进行归一化处理，将像素值缩放到0-1之间\n\n        return image, self.tmplabel\n        # 返回处理后的图像和标签，这里的图像是一个PyTorch张量，标签是一个PyTorch张量，用于多标签分类任务","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:02.824680Z","iopub.execute_input":"2023-06-08T08:46:02.825041Z","iopub.status.idle":"2023-06-08T08:46:02.851628Z","shell.execute_reply.started":"2023-06-08T08:46:02.825005Z","shell.execute_reply":"2023-06-08T08:46:02.850667Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_fold_dls(df_train, df_valid):\n\n    ds_train = BirdDataset(\n        df_train, \n        sr = Config.SR,\n        duration = Config.DURATION,\n        augmentations = None,\n        train = True\n    )\n    ds_val = BirdDataset(\n        df_valid, \n        sr = Config.SR,\n        duration = Config.DURATION,\n        augmentations = None,\n        train = False\n    )\n    dl_train = DataLoader(ds_train, batch_size=Config.batch_size , shuffle=True, num_workers = 2)    \n    dl_val = DataLoader(ds_val, batch_size=Config.batch_size, num_workers = 2)\n    return dl_train, dl_val, ds_train, ds_val","metadata":{"papermill":{"duration":0.036289,"end_time":"2022-04-22T06:01:17.539606","exception":false,"start_time":"2022-04-22T06:01:17.503317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:02.990478Z","iopub.execute_input":"2023-06-08T08:46:02.993031Z","iopub.status.idle":"2023-06-08T08:46:03.003671Z","shell.execute_reply.started":"2023-06-08T08:46:02.992992Z","shell.execute_reply":"2023-06-08T08:46:03.002601Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"dl_train, dl_val, ds_train, ds_val = get_fold_dls(df_train, df_valid)","metadata":{"papermill":{"duration":0.584852,"end_time":"2022-04-22T06:01:18.338238","exception":false,"start_time":"2022-04-22T06:01:17.753386","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:03.029070Z","iopub.execute_input":"2023-06-08T08:46:03.031841Z","iopub.status.idle":"2023-06-08T08:46:03.039561Z","shell.execute_reply.started":"2023-06-08T08:46:03.031802Z","shell.execute_reply":"2023-06-08T08:46:03.038428Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau, OneCycleLR\n\ndef get_optimizer(lr, params):\n    model_optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, params), \n            lr=lr,\n            weight_decay=Config.weight_decay\n        )\n    interval = \"epoch\"\n    \n    lr_scheduler = CosineAnnealingWarmRestarts(\n                            model_optimizer, \n                            T_0=Config.epochs, \n                            T_mult=1, \n                            eta_min=1e-6, \n                            last_epoch=-1\n                        )\n\n    return {\n        \"optimizer\": model_optimizer, \n        \"lr_scheduler\": {\n            \"scheduler\": lr_scheduler,\n            \"interval\": interval,\n            \"monitor\": \"val_loss\",\n            \"frequency\": 1\n        }\n    }","metadata":{"papermill":{"duration":0.048043,"end_time":"2022-04-22T06:01:22.109544","exception":false,"start_time":"2022-04-22T06:01:22.061501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:03.044529Z","iopub.execute_input":"2023-06-08T08:46:03.047176Z","iopub.status.idle":"2023-06-08T08:46:03.057897Z","shell.execute_reply.started":"2023-06-08T08:46:03.047137Z","shell.execute_reply":"2023-06-08T08:46:03.056744Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from torchtoolbox.tools import mixup_data, mixup_criterion\nimport torch.nn as nn\nfrom torch.nn.functional import cross_entropy\nimport torchmetrics\nimport timm","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:03.063484Z","iopub.execute_input":"2023-06-08T08:46:03.066316Z","iopub.status.idle":"2023-06-08T08:46:03.370325Z","shell.execute_reply.started":"2023-06-08T08:46:03.066161Z","shell.execute_reply":"2023-06-08T08:46:03.369298Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics\n\ndef padded_cmap(solution, submission, padding_factor=5):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    new_rows = []\n    for i in range(padding_factor):\n        new_rows.append([1 for i in range(len(solution.columns))])\n    new_rows = pd.DataFrame(new_rows)\n    new_rows.columns = solution.columns\n    padded_solution = pd.concat([solution, new_rows]).reset_index(drop=True).copy()\n    padded_submission = pd.concat([submission, new_rows]).reset_index(drop=True).copy()\n    score = sklearn.metrics.average_precision_score(\n        padded_solution.values,\n        padded_submission.values,\n        average='macro',\n    )\n    return score\n\ndef map_score(solution, submission):\n    solution = solution#.drop(['row_id'], axis=1, errors='ignore')\n    submission = submission#.drop(['row_id'], axis=1, errors='ignore')\n    score = sklearn.metrics.average_precision_score(\n        solution.values,\n        submission.values,\n        average='micro',\n    )\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:03.381047Z","iopub.execute_input":"2023-06-08T08:46:03.383755Z","iopub.status.idle":"2023-06-08T08:46:03.397900Z","shell.execute_reply.started":"2023-06-08T08:46:03.383710Z","shell.execute_reply":"2023-06-08T08:46:03.396804Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"dummy = df_valid[birds].copy()\ndummy[birds] = np.random.rand(dummy.shape[0],dummy.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:03.403634Z","iopub.execute_input":"2023-06-08T08:46:03.406567Z","iopub.status.idle":"2023-06-08T08:46:03.498761Z","shell.execute_reply.started":"2023-06-08T08:46:03.406527Z","shell.execute_reply":"2023-06-08T08:46:03.497531Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"padded_cmap(df_valid[birds], dummy[birds], padding_factor = 5)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:03.500072Z","iopub.execute_input":"2023-06-08T08:46:03.500518Z","iopub.status.idle":"2023-06-08T08:46:04.427370Z","shell.execute_reply.started":"2023-06-08T08:46:03.500476Z","shell.execute_reply":"2023-06-08T08:46:04.426150Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.47528243207272347"},"metadata":{}}]},{"cell_type":"code","source":"padded_cmap(df_valid[birds], dummy[birds], padding_factor = 1)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:04.429114Z","iopub.execute_input":"2023-06-08T08:46:04.429596Z","iopub.status.idle":"2023-06-08T08:46:05.138262Z","shell.execute_reply.started":"2023-06-08T08:46:04.429553Z","shell.execute_reply":"2023-06-08T08:46:05.136881Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.21353161925724687"},"metadata":{}}]},{"cell_type":"code","source":"map_score(df_valid[birds], dummy[birds])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:05.140020Z","iopub.execute_input":"2023-06-08T08:46:05.140499Z","iopub.status.idle":"2023-06-08T08:46:05.386934Z","shell.execute_reply.started":"2023-06-08T08:46:05.140457Z","shell.execute_reply":"2023-06-08T08:46:05.385684Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.0037646377491044386"},"metadata":{}}]},{"cell_type":"code","source":"class BirdClefModel(pl.LightningModule):\n    # 定义一个名为BirdClefModel的PyTorch Lightning模块，继承自LightningModule类\n\n    def __init__(self, model_name=Config.model, num_classes = Config.num_classes, pretrained = Config.pretrained):\n        super().__init__()\n        # 初始化函数，接收预训练模型名称、分类数和预训练模型是否预训练等参数\n\n        self.num_classes = num_classes\n        # 存储分类数\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        # 使用timm库创建预训练模型\n\n        if 'res' in model_name:\n            self.in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Linear(self.in_features, num_classes)\n        elif 'dense' in model_name:\n            self.in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Linear(self.in_features, num_classes)\n        elif 'efficientnet' in model_name:\n            self.in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Sequential(\n                nn.Linear(self.in_features, num_classes)\n            )\n        # 根据预训练模型的类型，修改模型的最后一层，使其输出与分类数相等\n\n        self.loss_function = nn.BCEWithLogitsLoss() \n        # 定义损失函数，使用二分类交叉熵损失函数\n\n    def forward(self,images):\n        logits = self.backbone(images)\n        # 前向传递，计算模型的输出\n\n        return logits\n        \n    def configure_optimizers(self):\n        return get_optimizer(lr=Config.LR, params=self.parameters())\n        # 配置优化器，这里使用了自定义函数get_optimizer()，返回一个优化器对象\n\n    def train_with_mixup(self, X, y):\n        X, y_a, y_b, lam = mixup_data(X, y, alpha=Config.mixup_alpha)\n        y_pred = self(X)\n        loss_mixup = mixup_criterion(cross_entropy, y_pred, y_a, y_b, lam)\n        return loss_mixup\n        # 实现Mixup数据增强的训练方式，计算Mixup损失\n\n    def training_step(self, batch, batch_idx):\n        image, target = batch\n        # 获取数据集中的一个批次的图像和标签\n\n        if Config.use_mixup:\n            loss = self.train_with_mixup(image, target)\n        else:\n            y_pred = self(image)\n            loss = self.loss_function(y_pred,target)\n        # 如果使用Mixup数据增强，则调用train_with_mixup()函数计算损失；否则，直接计算损失\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        # 记录训练损失\n\n        return loss        \n\n    def validation_step(self, batch, batch_idx):\n        image, target = batch     \n        y_pred = self(image)\n        val_loss = self.loss_function(y_pred, target)\n        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n        # 计算验证损失，并记录到TensorBoard日志中\n\n        return {\"val_loss\": val_loss, \"logits\": y_pred, \"targets\": target}\n        # 返回验证损失、模型输出和真实标签\n\n    def train_dataloader(self):\n        return self._train_dataloader \n        # 返回训练数据集的数据加载器\n\n    def validation_dataloader(self):\n        return self._validation_dataloader\n        # 返回验证数据集的数据加载器\n\n    def validation_epoch_end(self,outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        return {'val_loss': avg_loss,'val_cmap':0}\n        # 计算验证集平均损失，并返回字典，其中包含验证损失和val_cmap（此处未使用）","metadata":{"papermill":{"duration":0.156714,"end_time":"2022-04-22T06:01:22.301564","exception":false,"start_time":"2022-04-22T06:01:22.14485","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:05.389826Z","iopub.execute_input":"2023-06-08T08:46:05.390749Z","iopub.status.idle":"2023-06-08T08:46:05.415660Z","shell.execute_reply.started":"2023-06-08T08:46:05.390688Z","shell.execute_reply":"2023-06-08T08:46:05.414271Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nimport gc\n\ndef run_training():\n    print(f\"Running training...\")\n    logger = None\n    \n    \n    dl_train, dl_val, ds_train, ds_val = get_fold_dls(df_train, df_valid)\n    \n    audio_model = BirdClefModel()\n\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=Config.PATIENCE, verbose= True, mode=\"min\")\n    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                          dirpath= \"/kaggle/working/exp1/\",\n                                      save_top_k=1,\n                                      save_last= True,\n                                      save_weights_only=True,\n                                      filename= f'./{Config.model}_loss',\n                                      verbose= True,\n                                      mode='min')\n    \n    callbacks_to_use = [checkpoint_callback,early_stop_callback]\n\n\n    trainer = pl.Trainer(\n        gpus=1,\n        val_check_interval=0.5,\n        deterministic=True,\n        max_epochs=Config.epochs,\n        logger=logger,\n        auto_lr_find=False,    \n        callbacks=callbacks_to_use,\n        precision=Config.PRECISION, accelerator=\"gpu\" \n    )\n\n    print(\"Running trainer.fit\")\n    trainer.fit(audio_model, train_dataloaders = dl_train, val_dataloaders = dl_val)                \n\n    gc.collect()\n    torch.cuda.empty_cache()\n","metadata":{"papermill":{"duration":0.052364,"end_time":"2022-04-22T06:01:22.708806","exception":false,"start_time":"2022-04-22T06:01:22.656442","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-06-08T08:46:05.417801Z","iopub.execute_input":"2023-06-08T08:46:05.421912Z","iopub.status.idle":"2023-06-08T08:46:05.434782Z","shell.execute_reply.started":"2023-06-08T08:46:05.421876Z","shell.execute_reply":"2023-06-08T08:46:05.433710Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T08:46:05.436983Z","iopub.execute_input":"2023-06-08T08:46:05.437775Z","iopub.status.idle":"2023-06-08T08:53:08.905746Z","shell.execute_reply.started":"2023-06-08T08:46:05.437699Z","shell.execute_reply":"2023-06-08T08:53:08.904611Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Running training...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n","output_type":"stream"},{"name":"stdout","text":"Running trainer.fit\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0bda6bcfc6047158fe50dfd5ec51973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]}]}